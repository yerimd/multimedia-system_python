{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TcySJWZ1jK69"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yerimd/multimedia-system_python/blob/main/MM_Speaker_Recognition_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcl2SpNpdVru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144be307-9dbb-4084-8c0a-8ac6ce5a47f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirement"
      ],
      "metadata": {
        "id": "ZGzusUvjrirC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # For additional installation of libraries not included in the colab main library\n",
        "# !pip install \"library_name\"\n",
        "!pip install torch\n",
        "# !pip install tensorflow\n",
        "# nicesin97@yonsei.ac.kr"
      ],
      "metadata": {
        "id": "DiZR7OJxrmy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "539b927b-6458-4231-de29-6e549786deab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd '/content/drive/My Drive/Colab Notebooks/MM'"
      ],
      "metadata": {
        "id": "QbWeuznkt2xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7c1fe6-f3dd-4a9b-e33d-c0a728cd225b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/MM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import shutil\n",
        "import pickle\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from barbar import Bar\n",
        "import scipy.io.wavfile as wav\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "e7TAYwTCLozh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration"
      ],
      "metadata": {
        "id": "9T8Qkxy0sr6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Configuration for train_interface\n",
        "\n",
        "You can check the essential information,\n",
        "and if you want to change model structure or training method,\n",
        "you have to change this file.\n",
        "\"\"\"\n",
        "#######################################################################\n",
        "#                                 path                                #\n",
        "#######################################################################\n",
        "job_dir = './'  # 'FILE PATH for saving models'\n",
        "chkpt_model = None  # 'FILE PATH (if you have pretrained model..)'\n",
        "chkpt = str(\"EPOCH\")\n",
        "if chkpt_model is not None:\n",
        "    chkpt_path = job_dir + chkpt_model + '/chkpt_' + chkpt + '.pt'\n",
        "\n",
        "#######################################################################\n",
        "#                         possible setting                            #\n",
        "#######################################################################\n",
        "# the list you can do\n",
        "model_list = ['SPK_RECOG_MODEL', 'CRN']\n",
        "loss_list = ['Categorical_CE', 'MSE', 'SDR', 'SI-SNR', 'SI-SDR']\n",
        "\n",
        "# experiment number setting\n",
        "expr_name = 'Spk_Recog_14'\n",
        "DEVICE = 'cuda'  # if you want to run the code with 'cpu', change 'cpu'\n",
        "#######################################################################\n",
        "#                          Experimental setting                       #\n",
        "#######################################################################\n",
        "current_model = model_list[0]\n",
        "current_loss = loss_list[0]\n",
        "\n",
        "# hyper-parameters\n",
        "max_epochs = 400\n",
        "learning_rate = 0.0001\n",
        "batch = 16   # 4, 8, 16, 32, 64\n",
        "\n",
        "#######################################################################\n",
        "#                         model information                           #\n",
        "#######################################################################\n",
        "fs = 16000\n",
        "fft_len = 512       # 1024 # 2048\n",
        "sam_sec = fft_len / fs\n",
        "frm_samp = fs * (fft_len / fs)\n",
        "\n",
        "rnn_layers = 2\n",
        "rnn_input_size = 512\n",
        "rnn_units = 128\n",
        "#######################################################################\n",
        "#                      setting error check                            #\n",
        "#######################################################################\n",
        "# if the setting is wrong, print error message\n",
        "\n",
        "#######################################################################\n",
        "#                           print setting                             #\n",
        "#######################################################################\n",
        "print('--------------------  C  O  N  F  I  G  ----------------------')\n",
        "print('--------------------------------------------------------------')\n",
        "print('MODEL INFO : {}'.format(current_model))\n",
        "print('LOSS INFO : {}'.format(current_loss))\n",
        "print('\\nBATCH : {}'.format(batch))\n",
        "print('LEARNING RATE : {}'.format(learning_rate))\n",
        "print('--------------------------------------------------------------')\n",
        "print('--------------------------------------------------------------\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bqUaQ-SsrpD",
        "outputId": "025960da-125e-4a7a-eb31-38b6f04c2901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------  C  O  N  F  I  G  ----------------------\n",
            "--------------------------------------------------------------\n",
            "MODEL INFO : SPK_RECOG_MODEL\n",
            "LOSS INFO : Categorical_CE\n",
            "\n",
            "BATCH : 16\n",
            "LEARNING RATE : 0.0001\n",
            "--------------------------------------------------------------\n",
            "--------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Extract"
      ],
      "metadata": {
        "id": "pkWQs11PlHOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "n_spk = 26\n",
        "\n",
        "# 임의의 오디오를 특정 길이가 되도록 padding 하는 함수\n",
        "def pad_audio(audio):\n",
        "    length = len(audio)\n",
        "    return np.concatenate((audio, np.zeros((max_length-length))))\n",
        "\n",
        "\n",
        "# 가장 긴 길이의 오디오를 찾는다.\n",
        "# for문을 돌려서 모든 데이터의 길이를 length라는 리스트 안에 저장합니다\n",
        "# 화자 22명에 대하여 수행\n",
        "length = []\n",
        "for s in range(0, n_spk, 1):\n",
        "    for n in range(1, 6, 1):\n",
        "        (x,fs) = librosa.load('./data/speaker_data_wav_16k/'+str(s+1)+'_'+str(n) + '.wav')\n",
        "        length.append(len(x))\n",
        "        print(str(s)+'_'+str(n)+'번째 길이 탐색 완료!')\n",
        "\n",
        "max_length = np.max(length) # length라는 리스트 안에서 가장 긴 오디오의 길이를 리턴합니다\n",
        "print('가장 긴 오디오의 길이는 :', max_length/16000, 'seconds')\n",
        "\n",
        "pickle.dump(max_length, open('./data/max_length', 'wb'), protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "p6NsF9UxlJ5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22bafba3-eaf5-4d10-c5ff-2ae251c9cd7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0_1번째 길이 탐색 완료!\n",
            "0_2번째 길이 탐색 완료!\n",
            "0_3번째 길이 탐색 완료!\n",
            "0_4번째 길이 탐색 완료!\n",
            "0_5번째 길이 탐색 완료!\n",
            "1_1번째 길이 탐색 완료!\n",
            "1_2번째 길이 탐색 완료!\n",
            "1_3번째 길이 탐색 완료!\n",
            "1_4번째 길이 탐색 완료!\n",
            "1_5번째 길이 탐색 완료!\n",
            "2_1번째 길이 탐색 완료!\n",
            "2_2번째 길이 탐색 완료!\n",
            "2_3번째 길이 탐색 완료!\n",
            "2_4번째 길이 탐색 완료!\n",
            "2_5번째 길이 탐색 완료!\n",
            "3_1번째 길이 탐색 완료!\n",
            "3_2번째 길이 탐색 완료!\n",
            "3_3번째 길이 탐색 완료!\n",
            "3_4번째 길이 탐색 완료!\n",
            "3_5번째 길이 탐색 완료!\n",
            "4_1번째 길이 탐색 완료!\n",
            "4_2번째 길이 탐색 완료!\n",
            "4_3번째 길이 탐색 완료!\n",
            "4_4번째 길이 탐색 완료!\n",
            "4_5번째 길이 탐색 완료!\n",
            "5_1번째 길이 탐색 완료!\n",
            "5_2번째 길이 탐색 완료!\n",
            "5_3번째 길이 탐색 완료!\n",
            "5_4번째 길이 탐색 완료!\n",
            "5_5번째 길이 탐색 완료!\n",
            "6_1번째 길이 탐색 완료!\n",
            "6_2번째 길이 탐색 완료!\n",
            "6_3번째 길이 탐색 완료!\n",
            "6_4번째 길이 탐색 완료!\n",
            "6_5번째 길이 탐색 완료!\n",
            "7_1번째 길이 탐색 완료!\n",
            "7_2번째 길이 탐색 완료!\n",
            "7_3번째 길이 탐색 완료!\n",
            "7_4번째 길이 탐색 완료!\n",
            "7_5번째 길이 탐색 완료!\n",
            "8_1번째 길이 탐색 완료!\n",
            "8_2번째 길이 탐색 완료!\n",
            "8_3번째 길이 탐색 완료!\n",
            "8_4번째 길이 탐색 완료!\n",
            "8_5번째 길이 탐색 완료!\n",
            "9_1번째 길이 탐색 완료!\n",
            "9_2번째 길이 탐색 완료!\n",
            "9_3번째 길이 탐색 완료!\n",
            "9_4번째 길이 탐색 완료!\n",
            "9_5번째 길이 탐색 완료!\n",
            "10_1번째 길이 탐색 완료!\n",
            "10_2번째 길이 탐색 완료!\n",
            "10_3번째 길이 탐색 완료!\n",
            "10_4번째 길이 탐색 완료!\n",
            "10_5번째 길이 탐색 완료!\n",
            "11_1번째 길이 탐색 완료!\n",
            "11_2번째 길이 탐색 완료!\n",
            "11_3번째 길이 탐색 완료!\n",
            "11_4번째 길이 탐색 완료!\n",
            "11_5번째 길이 탐색 완료!\n",
            "12_1번째 길이 탐색 완료!\n",
            "12_2번째 길이 탐색 완료!\n",
            "12_3번째 길이 탐색 완료!\n",
            "12_4번째 길이 탐색 완료!\n",
            "12_5번째 길이 탐색 완료!\n",
            "13_1번째 길이 탐색 완료!\n",
            "13_2번째 길이 탐색 완료!\n",
            "13_3번째 길이 탐색 완료!\n",
            "13_4번째 길이 탐색 완료!\n",
            "13_5번째 길이 탐색 완료!\n",
            "14_1번째 길이 탐색 완료!\n",
            "14_2번째 길이 탐색 완료!\n",
            "14_3번째 길이 탐색 완료!\n",
            "14_4번째 길이 탐색 완료!\n",
            "14_5번째 길이 탐색 완료!\n",
            "15_1번째 길이 탐색 완료!\n",
            "15_2번째 길이 탐색 완료!\n",
            "15_3번째 길이 탐색 완료!\n",
            "15_4번째 길이 탐색 완료!\n",
            "15_5번째 길이 탐색 완료!\n",
            "16_1번째 길이 탐색 완료!\n",
            "16_2번째 길이 탐색 완료!\n",
            "16_3번째 길이 탐색 완료!\n",
            "16_4번째 길이 탐색 완료!\n",
            "16_5번째 길이 탐색 완료!\n",
            "17_1번째 길이 탐색 완료!\n",
            "17_2번째 길이 탐색 완료!\n",
            "17_3번째 길이 탐색 완료!\n",
            "17_4번째 길이 탐색 완료!\n",
            "17_5번째 길이 탐색 완료!\n",
            "18_1번째 길이 탐색 완료!\n",
            "18_2번째 길이 탐색 완료!\n",
            "18_3번째 길이 탐색 완료!\n",
            "18_4번째 길이 탐색 완료!\n",
            "18_5번째 길이 탐색 완료!\n",
            "19_1번째 길이 탐색 완료!\n",
            "19_2번째 길이 탐색 완료!\n",
            "19_3번째 길이 탐색 완료!\n",
            "19_4번째 길이 탐색 완료!\n",
            "19_5번째 길이 탐색 완료!\n",
            "20_1번째 길이 탐색 완료!\n",
            "20_2번째 길이 탐색 완료!\n",
            "20_3번째 길이 탐색 완료!\n",
            "20_4번째 길이 탐색 완료!\n",
            "20_5번째 길이 탐색 완료!\n",
            "21_1번째 길이 탐색 완료!\n",
            "21_2번째 길이 탐색 완료!\n",
            "21_3번째 길이 탐색 완료!\n",
            "21_4번째 길이 탐색 완료!\n",
            "21_5번째 길이 탐색 완료!\n",
            "22_1번째 길이 탐색 완료!\n",
            "22_2번째 길이 탐색 완료!\n",
            "22_3번째 길이 탐색 완료!\n",
            "22_4번째 길이 탐색 완료!\n",
            "22_5번째 길이 탐색 완료!\n",
            "23_1번째 길이 탐색 완료!\n",
            "23_2번째 길이 탐색 완료!\n",
            "23_3번째 길이 탐색 완료!\n",
            "23_4번째 길이 탐색 완료!\n",
            "23_5번째 길이 탐색 완료!\n",
            "24_1번째 길이 탐색 완료!\n",
            "24_2번째 길이 탐색 완료!\n",
            "24_3번째 길이 탐색 완료!\n",
            "24_4번째 길이 탐색 완료!\n",
            "24_5번째 길이 탐색 완료!\n",
            "25_1번째 길이 탐색 완료!\n",
            "25_2번째 길이 탐색 완료!\n",
            "25_3번째 길이 탐색 완료!\n",
            "25_4번째 길이 탐색 완료!\n",
            "25_5번째 길이 탐색 완료!\n",
            "가장 긴 오디오의 길이는 : 40.1895 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mode = 'devel'    # train / devel / test\n",
        "if mode == 'train':\n",
        "    start_idx = 1\n",
        "    end_idx = 5\n",
        "elif mode == 'devel':\n",
        "    start_idx = 5\n",
        "    end_idx = 6\n",
        "\n",
        "# 각 speaker들의 1~4번 음원에 대하여 train dataset으로 사용.\n",
        "# train_mfcc_features / train_labels로 저장\n",
        "print('Feature extraction for training data...')\n",
        "data_train = []\n",
        "refer_train = []\n",
        "for s in range(0, n_spk, 1):\n",
        "    for n in range(start_idx, end_idx, 1):\n",
        "        (x, fs) = librosa.load('./data/speaker_data_wav_16k/' + str(s+1) + '_' + str(n) + '.wav')\n",
        "        x = pad_audio(x)\n",
        "        mfcc = librosa.feature.mfcc(y=x, sr=fs, n_mfcc=13)\n",
        "        mfccs = mfcc - np.reshape(np.mean(mfcc, 1), (13, 1))  # Remove DC components # 13 x frame_size\n",
        "        mfccs = np.transpose(mfccs)\n",
        "        # labels = np.zeros((mfccs.shape[0], 22))\n",
        "        # labels[:, s - 1] = 1\n",
        "        labels = np.ones((1,))*s           # [1, 1]\n",
        "        data_train.append(np.float16(mfccs))\n",
        "        refer_train.append(np.float16(labels))      # [Batch, 1]\n",
        "        print(str(s) + '_' + str(n) + '번째 완료!')\n",
        "\n",
        "pickle.dump(np.array(data_train), open('./data/mfcc_features_' + str(mode), 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
        "pickle.dump(np.array(refer_train), open('./data/labels_' + str(mode), 'wb'), protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "ae5pWqbylSGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f90bc1b-8cc8-4ada-fade-dcdbb960c398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature extraction for training data...\n",
            "0_5번째 완료!\n",
            "1_5번째 완료!\n",
            "2_5번째 완료!\n",
            "3_5번째 완료!\n",
            "4_5번째 완료!\n",
            "5_5번째 완료!\n",
            "6_5번째 완료!\n",
            "7_5번째 완료!\n",
            "8_5번째 완료!\n",
            "9_5번째 완료!\n",
            "10_5번째 완료!\n",
            "11_5번째 완료!\n",
            "12_5번째 완료!\n",
            "13_5번째 완료!\n",
            "14_5번째 완료!\n",
            "15_5번째 완료!\n",
            "16_5번째 완료!\n",
            "17_5번째 완료!\n",
            "18_5번째 완료!\n",
            "19_5번째 완료!\n",
            "20_5번째 완료!\n",
            "21_5번째 완료!\n",
            "22_5번째 완료!\n",
            "23_5번째 완료!\n",
            "24_5번째 완료!\n",
            "25_5번째 완료!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIZVwIZshjAe"
      },
      "source": [
        "# 훈련 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tResuhrMdRS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f953e952-40a3-406b-8adf-7c0acf85d6a9"
      },
      "source": [
        "# Load data (Training data & Validation data)\n",
        "print('Load data...')\n",
        "X_train = pickle.load(open('./data/mfcc_features_train', 'rb'))\n",
        "Y_train = pickle.load(open('./data/labels_train', 'rb'))\n",
        "\n",
        "X_valid = pickle.load(open('./data/mfcc_features_devel', 'rb'))\n",
        "Y_valid = pickle.load(open('./data/labels_devel', 'rb'))\n",
        "\n",
        "\n",
        "# Validation용 데이터 만들기 (training set의 20%를 validation set으로 설정)\n",
        "print(X_train.shape)\n",
        "print(X_valid.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load data...\n",
            "(104, 1256, 13)\n",
            "(26, 1256, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "Wh-ojxMEu1Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# clip padding\n",
        "max_length = pickle.load(open('./data/max_length', 'rb'))\n",
        "\n",
        "# Reshape\n",
        "X_train = np.transpose(X_train, (0, 2, 1))\n",
        "X_valid = np.transpose(X_valid, (0, 2, 1))\n",
        "\n",
        "\n",
        "def create_dataloader(mode):\n",
        "    if mode == 'train':\n",
        "        return DataLoader(\n",
        "            dataset=Wave_Dataset(mode),\n",
        "            batch_size=batch,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=True,\n",
        "            drop_last=True,\n",
        "            sampler=None\n",
        "        )\n",
        "    elif mode == 'devel':\n",
        "        return DataLoader(\n",
        "            dataset=Wave_Dataset(mode),\n",
        "            batch_size=batch, shuffle=False, num_workers=0\n",
        "        )\n",
        "\n",
        "\n",
        "class Wave_Dataset(Dataset):\n",
        "    def __init__(self, mode):\n",
        "        # load data\n",
        "        if mode == 'train':\n",
        "            print('<Training dataset>')\n",
        "            print('Load the data...')\n",
        "            self.input = X_train\n",
        "            self.target = Y_train\n",
        "        elif mode == 'devel':\n",
        "            print('<Validation dataset>')\n",
        "            print('Load the data...')\n",
        "            self.input = X_valid\n",
        "            self.target = Y_valid\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "            inputs = self.input[idx]\n",
        "            targets = self.target[idx]\n",
        "\n",
        "            # transform to torch from numpy\n",
        "            inputs = torch.from_numpy(inputs)\n",
        "            targets = torch.from_numpy(targets)\n",
        "            return inputs, targets[0]"
      ],
      "metadata": {
        "id": "DyN7KlD8uwEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3jeaGrZhnRO"
      },
      "source": [
        "#모델 설계\n",
        "\n",
        "학습 데이터는 모두 준비되었고, 이제 학습을 위한 모델을 설계해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkYQpRgOhiQc"
      },
      "source": [
        "class SPK_RECOG_MODEL( nn.Module ):\n",
        "    def __init__(self):\n",
        "        super(SPK_RECOG_MODEL, self).__init__()\n",
        "\n",
        "        self.conv_layer1 = nn.Sequential(\n",
        "            nn.Conv1d(13, 32, kernel_size=3, padding=1, stride=2, dilation=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.conv_layer2 = nn.Sequential(\n",
        "            nn.Conv1d(32, 64, kernel_size=3, padding=1, stride=2, dilation=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.conv_layer3 = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=1, stride=2, dilation=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.conv_layer4 = nn.Sequential(\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=1, stride=2, dilation=1),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "        self.conv_layer5 = nn.Sequential(\n",
        "            nn.Conv1d(256, 256, kernel_size=3, padding=1, stride=2, dilation=1),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(256*20, 512)\n",
        "        self.act_fc1 = nn.PReLU()\n",
        "\n",
        "        self.fc2 = nn.Linear(512,256)\n",
        "        self.act_fc2 = nn.PReLU()\n",
        "\n",
        "        self.fc3 = nn.Linear(256, 26)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # CNN layers\n",
        "        x1 = self.conv_layer1(x)  # (B, 13, 1256) --> (B, 32, 628)\n",
        "        x2 = self.conv_layer2(x1) # (B, 32, 628) --> (B, 64, 314)\n",
        "        x3 = self.conv_layer3(x2)  # (B, 64, 314) --> (B, 128, 157)\n",
        "        x4 = self.conv_layer4(x3)  # (B, 128, 157) --> (B, 256, 79)\n",
        "        x5 = self.conv_layer5(x4)  # (B, 256, 79) --> (B, 256, 20)\n",
        "\n",
        "        # Flattening\n",
        "        x_flat = self.flatten(x5)\n",
        "\n",
        "        # FCN layers\n",
        "        x6 = self.fc1(x_flat)\n",
        "        x6 = self.act_fc1(x6)\n",
        "\n",
        "        x7 = self.fc2(x6)\n",
        "        x7 = self.act_fc2(x7)\n",
        "\n",
        "        x8 = self.fc3(x7)\n",
        "\n",
        "        return x8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "T_U4-WNREqx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "#                             For train                               #\n",
        "#######################################################################\n",
        "def model_train(model, optimizer, train_loader, DEVICE, criterion):\n",
        "    # initialization\n",
        "    train_loss = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    for inputs, labels in Bar( train_loader ):\n",
        "        batch_num += 1\n",
        "\n",
        "        # to cuda\n",
        "        inputs = inputs.float().to(DEVICE)\n",
        "        labels = labels.float().to(DEVICE)  # labels.shape [B]\n",
        "\n",
        "        est = model(inputs)  # est.shape [B, 26]\n",
        "        loss = F.cross_entropy(est, labels.long())\n",
        "\n",
        "        # # if you want to check the scale of the loss\n",
        "        # print('loss: {:.4}'.format(loss))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss\n",
        "    train_loss /= batch_num\n",
        "\n",
        "    return train_loss\n",
        "\n",
        "\n",
        "#######################################################################\n",
        "#                           For validation                            #\n",
        "#######################################################################\n",
        "def model_validate(model, validation_loader, dir_to_save, epoch, DEVICE, criterion):\n",
        "    # initialization\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    batch_num = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in Bar( validation_loader ):\n",
        "            batch_num += 1\n",
        "\n",
        "            # to cuda\n",
        "            inputs = inputs.float().to(DEVICE)\n",
        "            labels = labels.float().to(DEVICE)\n",
        "\n",
        "            est = model(inputs)                         # est.shape = [Batch, Num_of_Spk] (batch, 22)\n",
        "            loss = F.cross_entropy(est, labels.long())\n",
        "\n",
        "            # Calculate Accuracy\n",
        "            ans, est_idx = torch.max(est, 1)\n",
        "            correct += est_idx.eq(labels).sum().item()  #\n",
        "\n",
        "            validation_loss += loss\n",
        "\n",
        "        validation_loss /= batch_num\n",
        "        valid_acc = 100. * correct / len(validation_loader.dataset)\n",
        "\n",
        "        return validation_loss, valid_acc\n"
      ],
      "metadata": {
        "id": "zNXm6QT0EsYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Interface\n",
        "\n",
        "모델 훈련\n",
        "\n",
        "이제, 학습을 시작합니다.\n",
        "\n",
        "Epoch은 전체 데이터에 대한 반복 학습 횟수입니다.\n",
        "\n",
        "학습 도중 각 단계마다 오차(loss)와 정확도(acc)가 출력됩니다.\n",
        "\n",
        "학습이 진행되는 동안 이 둘이 어떻게 변하는지 확인합니다.\n",
        "\n",
        "매 Epoch마다 체크 포인트를 발동하여 weights정보를 기록합니다. 가장 validation accuracy가 높았을 때의 weights를 저장하기 위합니다.\n"
      ],
      "metadata": {
        "id": "p0dgiKMvG9Ss"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeHpBMc4A2QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "#                        Helper function definition                           #\n",
        "###############################################################################\n",
        "# Write training related parameters into the log file.\n",
        "def write_status_to_log_file(fp, total_parameters):\n",
        "    fp.write('%d-%d-%d %d:%d:%d\\n' %\n",
        "             (time.localtime().tm_year, time.localtime().tm_mon,\n",
        "              time.localtime().tm_mday, time.localtime().tm_hour,\n",
        "              time.localtime().tm_min, time.localtime().tm_sec))\n",
        "    fp.write('total params   : %d (%.2f M, %.2f MBytes)\\n' %\n",
        "             (total_parameters,\n",
        "              total_parameters / 1000000.0,\n",
        "              total_parameters * 4.0 / 1000000.0))\n",
        "\n",
        "\n",
        "# Calculate the size of total network.\n",
        "def calculate_total_params(our_model):\n",
        "    total_parameters = 0\n",
        "    for variable in our_model.parameters():\n",
        "        shape = variable.size()\n",
        "        variable_parameters = 1\n",
        "        for dim in shape:\n",
        "            variable_parameters *= dim\n",
        "        total_parameters += variable_parameters\n",
        "\n",
        "    return total_parameters\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "#         Parameter Initialization and Setting for model training             #\n",
        "###############################################################################\n",
        "# Set device\n",
        "DEVICE = torch.device('cuda') # if you want to run the code with 'cpu', change 'cpu'\n",
        "\n",
        "# Set model\n",
        "model = SPK_RECOG_MODEL().to(DEVICE)\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "total_params = calculate_total_params(model)\n",
        "\n",
        "###############################################################################\n",
        "#                        Confirm model information                            #\n",
        "###############################################################################\n",
        "print('%d-%d-%d %d:%d:%d\\n' %\n",
        "      (time.localtime().tm_year, time.localtime().tm_mon,\n",
        "       time.localtime().tm_mday, time.localtime().tm_hour,\n",
        "       time.localtime().tm_min, time.localtime().tm_sec))\n",
        "print('total params   : %d (%.2f M, %.2f MBytes)\\n' %\n",
        "      (total_params,\n",
        "       total_params / 1000000.0,\n",
        "       total_params * 4.0 / 1000000.0))\n",
        "\n",
        "###############################################################################\n",
        "#                              Create Dataloader                              #\n",
        "###############################################################################\n",
        "train_loader = create_dataloader(mode='train')\n",
        "validation_loader = create_dataloader(mode='devel')\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(torch.ones(22)).to(DEVICE)\n",
        "# criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "###############################################################################\n",
        "#                        Set a log file to store progress.                    #\n",
        "#               Set a hps file to store hyper-parameters information.         #\n",
        "###############################################################################\n",
        "if chkpt_model is not None:  # Load the checkpoint\n",
        "    print('Resuming from checkpoint: %s' % chkpt_path)\n",
        "\n",
        "    # Set a log file to store progress.\n",
        "    dir_to_save = job_dir + chkpt_model\n",
        "\n",
        "    checkpoint = torch.load(chkpt_path)\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    epoch_start_idx = checkpoint['epoch'] + 1\n",
        "    acc_vali_total = np.load(str(dir_to_save + '/acc_vali_total.npy'))\n",
        "else:  # First learning\n",
        "    print('Starting new training run...')\n",
        "    epoch_start_idx = 1\n",
        "    acc_vali_total = np.zeros(max_epochs)\n",
        "\n",
        "    # Set a log file to store progress.\n",
        "    dir_to_save = job_dir + expr_name + '_%d.%d' % (time.localtime().tm_mon,\n",
        "                                                           time.localtime().tm_mday) + '_%s' % current_model + '_%s' % current_loss\n",
        "\n",
        "# make the file directory\n",
        "if not os.path.exists(dir_to_save):\n",
        "    os.mkdir(dir_to_save)\n",
        "\n",
        "# logging\n",
        "log_fname = str(dir_to_save + '/log.txt')\n",
        "fp = open(log_fname, 'w')\n",
        "write_status_to_log_file(fp, total_params)\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "#                             Main program start !!                           #\n",
        "###############################################################################\n",
        "###############################################################################\n",
        "\n",
        "###############################################################################\n",
        "#                                    Train                                    #\n",
        "###############################################################################\n",
        "acc_valid_total = np.zeros(max_epochs)\n",
        "\n",
        "for epoch in range(epoch_start_idx, max_epochs):\n",
        "        start_time = time.time()\n",
        "        # Training\n",
        "        train_loss = model_train(model, optimizer, train_loader, DEVICE, criterion)\n",
        "\n",
        "        # save checkpoint file to resume training\n",
        "        save_path = str(dir_to_save + '/' + ('chkpt_%d.pt' % epoch))\n",
        "\n",
        "        torch.save({\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'epoch': epoch\n",
        "        }, save_path)\n",
        "\n",
        "        # Validation\n",
        "        valid_loss, valid_acc = \\\n",
        "        model_validate(model, validation_loader, dir_to_save, epoch, DEVICE, criterion)\n",
        "\n",
        "        print('Epoch [{}] | T {:.6f} | V {:.4} V_ACC {:.2f} takes {:.2f} seconds\\n'\n",
        "                  .format(epoch, train_loss, valid_loss, valid_acc, time.time() - start_time))\n",
        "        # log file save\n",
        "        fp.write('Epoch [{}] | T {:.6f} | V {:.4} V_ACC {:.2f} takes {:.2f} seconds\\n'\n",
        "                     .format(epoch, train_loss, valid_loss, valid_acc, time.time() - start_time))\n",
        "\n",
        "        acc_valid_total[epoch - 1] = valid_acc\n",
        "        np.save(str(dir_to_save + '/acc_valid_total.npy'), acc_valid_total)\n",
        "\n",
        "\n",
        "fp.close()\n",
        "print('Training has been finished.')\n",
        "\n",
        "# Copy optimum model that has minimum MSE.\n",
        "print('Save optimum models...')\n",
        "max_index = np.argmax(acc_valid_total)\n",
        "print('Maximum Accuracy is at ' + str(max_index + 1) + '.')\n",
        "src_file = str(dir_to_save + '/' + ('chkpt_%d.pt' % (max_index + 1)))\n",
        "tgt_file = str(dir_to_save + '/chkpt_opt.pt')\n",
        "shutil.copy(src_file, tgt_file)"
      ],
      "metadata": {
        "id": "RRpgtUvjG_4T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "50980a4f-b53b-4f5b-81f0-6e7ab2237054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-6-12 8:23:5\n",
            "\n",
            "total params   : 3088544 (3.09 M, 12.35 MBytes)\n",
            "\n",
            "<Training dataset>\n",
            "Load the data...\n",
            "<Validation dataset>\n",
            "Load the data...\n",
            "Starting new training run...\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [1] | T 3.269101 | V 3.252 V_ACC 3.85 takes 0.21 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [2] | T 3.157446 | V 3.232 V_ACC 15.38 takes 0.18 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [3] | T 3.022184 | V 3.185 V_ACC 34.62 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [4] | T 2.858277 | V 3.082 V_ACC 50.00 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [5] | T 2.603779 | V 2.898 V_ACC 50.00 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [6] | T 2.295048 | V 2.614 V_ACC 50.00 takes 0.25 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [7] | T 1.923483 | V 2.271 V_ACC 53.85 takes 0.22 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [8] | T 1.511791 | V 1.935 V_ACC 65.38 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [9] | T 1.159325 | V 1.584 V_ACC 73.08 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [10] | T 0.802751 | V 1.348 V_ACC 76.92 takes 0.21 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [11] | T 0.550889 | V 1.058 V_ACC 92.31 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [12] | T 0.337920 | V 0.9087 V_ACC 84.62 takes 0.21 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [13] | T 0.236248 | V 0.8297 V_ACC 88.46 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [14] | T 0.148953 | V 0.7537 V_ACC 88.46 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [15] | T 0.107433 | V 0.6516 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [16] | T 0.084681 | V 0.6236 V_ACC 88.46 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [17] | T 0.066412 | V 0.5772 V_ACC 88.46 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [18] | T 0.047228 | V 0.5433 V_ACC 88.46 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [19] | T 0.044299 | V 0.5154 V_ACC 92.31 takes 0.21 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [20] | T 0.039241 | V 0.5201 V_ACC 88.46 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [21] | T 0.027533 | V 0.5484 V_ACC 88.46 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [22] | T 0.027362 | V 0.5181 V_ACC 88.46 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [23] | T 0.023285 | V 0.4839 V_ACC 92.31 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [24] | T 0.023022 | V 0.4797 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [25] | T 0.022062 | V 0.503 V_ACC 88.46 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [26] | T 0.018841 | V 0.4901 V_ACC 88.46 takes 0.43 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [27] | T 0.016152 | V 0.4482 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [28] | T 0.015799 | V 0.433 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [29] | T 0.014785 | V 0.4288 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [30] | T 0.014380 | V 0.4368 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [31] | T 0.014505 | V 0.4574 V_ACC 88.46 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [32] | T 0.011524 | V 0.4447 V_ACC 88.46 takes 0.19 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [33] | T 0.011095 | V 0.4362 V_ACC 88.46 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [34] | T 0.009910 | V 0.4274 V_ACC 88.46 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [35] | T 0.009368 | V 0.4202 V_ACC 88.46 takes 0.41 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [36] | T 0.008850 | V 0.4191 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [37] | T 0.008677 | V 0.4256 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [38] | T 0.007913 | V 0.4333 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [39] | T 0.007773 | V 0.4419 V_ACC 88.46 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [40] | T 0.007046 | V 0.4275 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [41] | T 0.007091 | V 0.4275 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [42] | T 0.006645 | V 0.4136 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [43] | T 0.006564 | V 0.3967 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [44] | T 0.005616 | V 0.395 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [45] | T 0.005892 | V 0.4017 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [46] | T 0.004937 | V 0.4057 V_ACC 88.46 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [47] | T 0.005365 | V 0.4076 V_ACC 88.46 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [48] | T 0.005614 | V 0.404 V_ACC 92.31 takes 0.40 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [49] | T 0.004817 | V 0.4041 V_ACC 92.31 takes 0.41 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [50] | T 0.004582 | V 0.4006 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [51] | T 0.004486 | V 0.3944 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [52] | T 0.004285 | V 0.3889 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [53] | T 0.004028 | V 0.3963 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [54] | T 0.004176 | V 0.3917 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [55] | T 0.003427 | V 0.3911 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [56] | T 0.003657 | V 0.3954 V_ACC 92.31 takes 0.66 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [57] | T 0.003677 | V 0.392 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [58] | T 0.003934 | V 0.3877 V_ACC 92.31 takes 0.40 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [59] | T 0.003130 | V 0.3881 V_ACC 92.31 takes 0.71 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [60] | T 0.003826 | V 0.4062 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [61] | T 0.003773 | V 0.3985 V_ACC 92.31 takes 0.74 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [62] | T 0.003139 | V 0.3972 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [63] | T 0.003649 | V 0.3906 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [64] | T 0.003210 | V 0.4 V_ACC 92.31 takes 0.41 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [65] | T 0.003050 | V 0.396 V_ACC 92.31 takes 0.54 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [66] | T 0.002833 | V 0.3924 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [67] | T 0.003503 | V 0.3845 V_ACC 92.31 takes 0.41 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [68] | T 0.002769 | V 0.3802 V_ACC 92.31 takes 0.64 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [69] | T 0.002965 | V 0.3763 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [70] | T 0.002666 | V 0.3831 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [71] | T 0.002764 | V 0.3891 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [72] | T 0.002347 | V 0.3869 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [73] | T 0.002294 | V 0.3928 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [74] | T 0.003112 | V 0.3884 V_ACC 92.31 takes 0.69 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [75] | T 0.002007 | V 0.3849 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [76] | T 0.002173 | V 0.3888 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [77] | T 0.002132 | V 0.3884 V_ACC 92.31 takes 0.66 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [78] | T 0.001712 | V 0.3886 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [79] | T 0.002144 | V 0.397 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [80] | T 0.001661 | V 0.3994 V_ACC 92.31 takes 0.62 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [81] | T 0.001758 | V 0.4039 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [82] | T 0.001733 | V 0.4028 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [83] | T 0.002033 | V 0.4058 V_ACC 92.31 takes 0.44 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [84] | T 0.001705 | V 0.4037 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [85] | T 0.001583 | V 0.3917 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [86] | T 0.001552 | V 0.384 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [87] | T 0.001439 | V 0.3813 V_ACC 92.31 takes 0.46 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [88] | T 0.001246 | V 0.3757 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [89] | T 0.001327 | V 0.3712 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [90] | T 0.001856 | V 0.3815 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [91] | T 0.001418 | V 0.3942 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [92] | T 0.001301 | V 0.3957 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [93] | T 0.001285 | V 0.3958 V_ACC 92.31 takes 0.41 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [94] | T 0.001319 | V 0.3945 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [95] | T 0.001421 | V 0.3926 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [96] | T 0.001324 | V 0.3874 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [97] | T 0.001561 | V 0.3873 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [98] | T 0.001133 | V 0.3859 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [99] | T 0.001396 | V 0.3879 V_ACC 92.31 takes 0.55 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [100] | T 0.001265 | V 0.3851 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [101] | T 0.001101 | V 0.3835 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [102] | T 0.001191 | V 0.3842 V_ACC 92.31 takes 0.54 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [103] | T 0.001078 | V 0.3831 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [104] | T 0.001131 | V 0.3847 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [105] | T 0.001022 | V 0.3788 V_ACC 92.31 takes 0.65 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [106] | T 0.000970 | V 0.3806 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [107] | T 0.001136 | V 0.3821 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [108] | T 0.001079 | V 0.3874 V_ACC 92.31 takes 0.49 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [109] | T 0.000978 | V 0.3862 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [110] | T 0.001109 | V 0.3956 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [111] | T 0.000821 | V 0.4004 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [112] | T 0.001001 | V 0.4022 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [113] | T 0.000906 | V 0.3959 V_ACC 92.31 takes 0.47 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [114] | T 0.000824 | V 0.3994 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [115] | T 0.000933 | V 0.4002 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [116] | T 0.000777 | V 0.3992 V_ACC 92.31 takes 0.43 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [117] | T 0.000971 | V 0.398 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [118] | T 0.000930 | V 0.3908 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [119] | T 0.001029 | V 0.3858 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [120] | T 0.000907 | V 0.3859 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [121] | T 0.000852 | V 0.382 V_ACC 92.31 takes 0.48 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [122] | T 0.000705 | V 0.3902 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [123] | T 0.000696 | V 0.3908 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [124] | T 0.000775 | V 0.3914 V_ACC 92.31 takes 0.55 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [125] | T 0.000836 | V 0.3835 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [126] | T 0.000884 | V 0.3891 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [127] | T 0.000719 | V 0.3891 V_ACC 92.31 takes 0.46 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [128] | T 0.000669 | V 0.3891 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [129] | T 0.000703 | V 0.3908 V_ACC 92.31 takes 0.63 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [130] | T 0.000758 | V 0.392 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [131] | T 0.000718 | V 0.3917 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [132] | T 0.000675 | V 0.3912 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [133] | T 0.000686 | V 0.3931 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [134] | T 0.000786 | V 0.3944 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [135] | T 0.000652 | V 0.4002 V_ACC 92.31 takes 0.63 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [136] | T 0.000748 | V 0.3989 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [137] | T 0.000709 | V 0.4018 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [138] | T 0.000672 | V 0.3936 V_ACC 92.31 takes 0.61 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [139] | T 0.000845 | V 0.3852 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [140] | T 0.000540 | V 0.3851 V_ACC 92.31 takes 0.65 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [141] | T 0.000664 | V 0.3853 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [142] | T 0.000837 | V 0.39 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [143] | T 0.000577 | V 0.4004 V_ACC 92.31 takes 0.58 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [144] | T 0.000712 | V 0.4087 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [145] | T 0.000513 | V 0.4127 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [146] | T 0.000676 | V 0.4111 V_ACC 92.31 takes 0.69 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [147] | T 0.000627 | V 0.4074 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [148] | T 0.000584 | V 0.415 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [149] | T 0.000824 | V 0.4191 V_ACC 92.31 takes 0.63 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [150] | T 0.000622 | V 0.4214 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [151] | T 0.000607 | V 0.4246 V_ACC 92.31 takes 0.73 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [152] | T 0.000491 | V 0.4246 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [153] | T 0.000503 | V 0.4174 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [154] | T 0.000587 | V 0.4089 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [155] | T 0.000520 | V 0.4077 V_ACC 92.31 takes 0.41 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [156] | T 0.000526 | V 0.4038 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [157] | T 0.000521 | V 0.4028 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [158] | T 0.000485 | V 0.3979 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [159] | T 0.000485 | V 0.3978 V_ACC 92.31 takes 0.42 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [160] | T 0.000453 | V 0.4015 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [161] | T 0.000505 | V 0.4044 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [162] | T 0.000447 | V 0.4084 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [163] | T 0.000601 | V 0.4061 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [164] | T 0.000385 | V 0.4014 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [165] | T 0.000410 | V 0.402 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [166] | T 0.000490 | V 0.399 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [167] | T 0.000384 | V 0.4055 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [168] | T 0.000459 | V 0.4027 V_ACC 92.31 takes 0.72 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [169] | T 0.000467 | V 0.4077 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [170] | T 0.000391 | V 0.4087 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [171] | T 0.000435 | V 0.3993 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [172] | T 0.000692 | V 0.4012 V_ACC 92.31 takes 0.55 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [173] | T 0.000356 | V 0.4123 V_ACC 92.31 takes 0.53 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [174] | T 0.000512 | V 0.4182 V_ACC 92.31 takes 0.48 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [175] | T 0.000938 | V 0.4223 V_ACC 92.31 takes 0.45 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [176] | T 0.000395 | V 0.4461 V_ACC 92.31 takes 0.42 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [177] | T 0.000417 | V 0.4442 V_ACC 92.31 takes 0.46 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [178] | T 0.000420 | V 0.4505 V_ACC 92.31 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [179] | T 0.000365 | V 0.4384 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [180] | T 0.000390 | V 0.4289 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [181] | T 0.000399 | V 0.4092 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [182] | T 0.000345 | V 0.408 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [183] | T 0.000366 | V 0.4065 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [184] | T 0.000320 | V 0.4034 V_ACC 92.31 takes 0.47 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [185] | T 0.000369 | V 0.3993 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [186] | T 0.000400 | V 0.4063 V_ACC 92.31 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [187] | T 0.000328 | V 0.4008 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [188] | T 0.000313 | V 0.4061 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [189] | T 0.000390 | V 0.4111 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [190] | T 0.000341 | V 0.4111 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [191] | T 0.000291 | V 0.4103 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [192] | T 0.000371 | V 0.4123 V_ACC 92.31 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [193] | T 0.000347 | V 0.4153 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [194] | T 0.000365 | V 0.4181 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [195] | T 0.000366 | V 0.4202 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [196] | T 0.000283 | V 0.416 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [197] | T 0.000280 | V 0.4133 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [198] | T 0.000333 | V 0.4099 V_ACC 92.31 takes 0.58 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [199] | T 0.000266 | V 0.4118 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [200] | T 0.000292 | V 0.4053 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [201] | T 0.000319 | V 0.4061 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [202] | T 0.000302 | V 0.4085 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [203] | T 0.000323 | V 0.4132 V_ACC 92.31 takes 0.59 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [204] | T 0.000254 | V 0.4082 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [205] | T 0.000285 | V 0.4093 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [206] | T 0.000369 | V 0.4105 V_ACC 92.31 takes 0.66 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [207] | T 0.000279 | V 0.4146 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [208] | T 0.000273 | V 0.4083 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [209] | T 0.000244 | V 0.4137 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [210] | T 0.000270 | V 0.4095 V_ACC 92.31 takes 0.45 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [211] | T 0.000296 | V 0.4086 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [212] | T 0.000271 | V 0.4097 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [213] | T 0.000244 | V 0.4092 V_ACC 92.31 takes 0.51 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [214] | T 0.000262 | V 0.4097 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [215] | T 0.000233 | V 0.4102 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [216] | T 0.000255 | V 0.4101 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [217] | T 0.000321 | V 0.4131 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [218] | T 0.000310 | V 0.4123 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [219] | T 0.000265 | V 0.4227 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [220] | T 0.000216 | V 0.4237 V_ACC 92.31 takes 0.68 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [221] | T 0.000316 | V 0.4231 V_ACC 92.31 takes 0.47 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [222] | T 0.000211 | V 0.4284 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [223] | T 0.000312 | V 0.425 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [224] | T 0.000214 | V 0.4296 V_ACC 92.31 takes 0.43 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [225] | T 0.000250 | V 0.4214 V_ACC 92.31 takes 0.25 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [226] | T 0.000238 | V 0.4149 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [227] | T 0.000234 | V 0.4095 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [228] | T 0.000238 | V 0.4097 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [229] | T 0.000244 | V 0.4094 V_ACC 92.31 takes 0.41 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [230] | T 0.000254 | V 0.4058 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [231] | T 0.000259 | V 0.4085 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [232] | T 0.000242 | V 0.4072 V_ACC 92.31 takes 0.60 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [233] | T 0.000250 | V 0.4051 V_ACC 92.31 takes 0.44 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [234] | T 0.000244 | V 0.4046 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [235] | T 0.000222 | V 0.4102 V_ACC 92.31 takes 0.45 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [236] | T 0.000222 | V 0.4117 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [237] | T 0.000222 | V 0.4165 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [238] | T 0.000179 | V 0.4198 V_ACC 92.31 takes 0.91 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [239] | T 0.000204 | V 0.4183 V_ACC 92.31 takes 0.44 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [240] | T 0.000237 | V 0.4194 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [241] | T 0.000180 | V 0.4257 V_ACC 92.31 takes 0.49 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [242] | T 0.000210 | V 0.424 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [243] | T 0.000271 | V 0.4288 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [244] | T 0.000203 | V 0.4299 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [245] | T 0.000221 | V 0.4247 V_ACC 92.31 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [246] | T 0.000209 | V 0.4216 V_ACC 92.31 takes 0.70 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [247] | T 0.000174 | V 0.4185 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [248] | T 0.000197 | V 0.4188 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [249] | T 0.000195 | V 0.4185 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [250] | T 0.000158 | V 0.4201 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [251] | T 0.000207 | V 0.4215 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [252] | T 0.000209 | V 0.4237 V_ACC 92.31 takes 0.43 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [253] | T 0.000239 | V 0.4292 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [254] | T 0.000181 | V 0.4183 V_ACC 92.31 takes 0.76 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [255] | T 0.000224 | V 0.4144 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [256] | T 0.000163 | V 0.415 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [257] | T 0.000177 | V 0.4146 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [258] | T 0.000172 | V 0.4141 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [259] | T 0.000224 | V 0.4217 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [260] | T 0.000172 | V 0.4165 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [261] | T 0.000165 | V 0.4205 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [262] | T 0.000153 | V 0.4203 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [263] | T 0.000172 | V 0.4199 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [264] | T 0.000198 | V 0.4196 V_ACC 92.31 takes 0.26 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [265] | T 0.000169 | V 0.4219 V_ACC 92.31 takes 0.21 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [266] | T 0.000157 | V 0.4258 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [267] | T 0.000147 | V 0.4211 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [268] | T 0.000157 | V 0.4238 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [269] | T 0.000176 | V 0.4206 V_ACC 92.31 takes 0.76 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [270] | T 0.000157 | V 0.4194 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [271] | T 0.000163 | V 0.4271 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [272] | T 0.000149 | V 0.4332 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [273] | T 0.000195 | V 0.4272 V_ACC 92.31 takes 0.44 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [274] | T 0.000156 | V 0.4323 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [275] | T 0.000192 | V 0.432 V_ACC 92.31 takes 0.72 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [276] | T 0.000176 | V 0.429 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [277] | T 0.000139 | V 0.4326 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [278] | T 0.000149 | V 0.4327 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [279] | T 0.000197 | V 0.4363 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [280] | T 0.000154 | V 0.4339 V_ACC 92.31 takes 0.80 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [281] | T 0.000140 | V 0.4287 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [282] | T 0.000153 | V 0.4262 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [283] | T 0.000161 | V 0.4256 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [284] | T 0.000178 | V 0.4227 V_ACC 92.31 takes 0.67 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [285] | T 0.000198 | V 0.4225 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [286] | T 0.000121 | V 0.429 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [287] | T 0.000196 | V 0.4292 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [288] | T 0.000153 | V 0.4244 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [289] | T 0.000131 | V 0.4226 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [290] | T 0.000137 | V 0.4209 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [291] | T 0.000174 | V 0.4239 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [292] | T 0.000133 | V 0.426 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [293] | T 0.000165 | V 0.4276 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [294] | T 0.000137 | V 0.4274 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [295] | T 0.000155 | V 0.4341 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [296] | T 0.000138 | V 0.4348 V_ACC 92.31 takes 0.66 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [297] | T 0.000128 | V 0.432 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [298] | T 0.000192 | V 0.4281 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [299] | T 0.000151 | V 0.4226 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [300] | T 0.000127 | V 0.4244 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [301] | T 0.000137 | V 0.421 V_ACC 92.31 takes 0.42 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [302] | T 0.000101 | V 0.4222 V_ACC 92.31 takes 0.51 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [303] | T 0.000113 | V 0.4306 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [304] | T 0.000160 | V 0.423 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [305] | T 0.000111 | V 0.4259 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [306] | T 0.000132 | V 0.4236 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [307] | T 0.000140 | V 0.426 V_ACC 92.31 takes 0.74 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [308] | T 0.000132 | V 0.4263 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [309] | T 0.000122 | V 0.4283 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [310] | T 0.000105 | V 0.4345 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [311] | T 0.000117 | V 0.4337 V_ACC 92.31 takes 0.70 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [312] | T 0.000119 | V 0.4305 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [313] | T 0.000145 | V 0.4291 V_ACC 92.31 takes 0.70 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [314] | T 0.000137 | V 0.4314 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [315] | T 0.000111 | V 0.4295 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [316] | T 0.000104 | V 0.4308 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [317] | T 0.000149 | V 0.432 V_ACC 92.31 takes 0.44 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [318] | T 0.000103 | V 0.4381 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [319] | T 0.000131 | V 0.4392 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [320] | T 0.000116 | V 0.437 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [321] | T 0.000094 | V 0.435 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [322] | T 0.000102 | V 0.4295 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [323] | T 0.000108 | V 0.4301 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [324] | T 0.000103 | V 0.4346 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [325] | T 0.000095 | V 0.4389 V_ACC 92.31 takes 0.45 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [326] | T 0.000125 | V 0.4301 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [327] | T 0.000144 | V 0.4357 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [328] | T 0.000131 | V 0.4334 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [329] | T 0.000106 | V 0.4353 V_ACC 92.31 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [330] | T 0.000118 | V 0.4353 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [331] | T 0.000123 | V 0.4346 V_ACC 92.31 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [332] | T 0.000114 | V 0.4347 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [333] | T 0.000100 | V 0.4357 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [334] | T 0.000107 | V 0.4339 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [335] | T 0.000133 | V 0.4334 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [336] | T 0.000098 | V 0.4378 V_ACC 92.31 takes 0.23 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [337] | T 0.000097 | V 0.4376 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [338] | T 0.000106 | V 0.4403 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [339] | T 0.000096 | V 0.4359 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [340] | T 0.000105 | V 0.4363 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [341] | T 0.000087 | V 0.438 V_ACC 92.31 takes 0.60 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [342] | T 0.000088 | V 0.4328 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [343] | T 0.000091 | V 0.4341 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [344] | T 0.000124 | V 0.4388 V_ACC 92.31 takes 0.51 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [345] | T 0.000125 | V 0.4394 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [346] | T 0.000106 | V 0.4381 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [347] | T 0.000068 | V 0.4418 V_ACC 92.31 takes 0.66 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [348] | T 0.000089 | V 0.4405 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [349] | T 0.000109 | V 0.4401 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [350] | T 0.000072 | V 0.4433 V_ACC 92.31 takes 0.22 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [351] | T 0.000080 | V 0.4429 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [352] | T 0.000085 | V 0.4432 V_ACC 92.31 takes 0.63 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [353] | T 0.000087 | V 0.4397 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [354] | T 0.000087 | V 0.4427 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [355] | T 0.000090 | V 0.443 V_ACC 92.31 takes 0.44 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [356] | T 0.000099 | V 0.443 V_ACC 92.31 takes 0.31 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [357] | T 0.000103 | V 0.445 V_ACC 92.31 takes 0.42 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [358] | T 0.000085 | V 0.4399 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [359] | T 0.000082 | V 0.4369 V_ACC 92.31 takes 0.37 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [360] | T 0.000088 | V 0.4364 V_ACC 92.31 takes 0.52 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [361] | T 0.000093 | V 0.4366 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [362] | T 0.000092 | V 0.4459 V_ACC 92.31 takes 0.40 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [363] | T 0.000077 | V 0.4427 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [364] | T 0.000086 | V 0.4393 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [365] | T 0.000106 | V 0.444 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [366] | T 0.000072 | V 0.4439 V_ACC 92.31 takes 0.54 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [367] | T 0.000086 | V 0.4434 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [368] | T 0.000067 | V 0.4441 V_ACC 92.31 takes 0.56 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [369] | T 0.000097 | V 0.4427 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [370] | T 0.000074 | V 0.4407 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [371] | T 0.000108 | V 0.4437 V_ACC 92.31 takes 0.66 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [372] | T 0.000072 | V 0.4435 V_ACC 92.31 takes 0.30 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [373] | T 0.000081 | V 0.4484 V_ACC 92.31 takes 0.38 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [374] | T 0.000090 | V 0.4454 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [375] | T 0.000079 | V 0.448 V_ACC 92.31 takes 0.32 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [376] | T 0.000078 | V 0.4447 V_ACC 92.31 takes 0.40 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [377] | T 0.000083 | V 0.4377 V_ACC 92.31 takes 0.34 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [378] | T 0.000075 | V 0.4368 V_ACC 92.31 takes 0.40 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [379] | T 0.000071 | V 0.4437 V_ACC 92.31 takes 0.40 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [380] | T 0.000061 | V 0.4466 V_ACC 92.31 takes 0.40 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [381] | T 0.000070 | V 0.45 V_ACC 92.31 takes 0.75 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [382] | T 0.000071 | V 0.445 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [383] | T 0.000082 | V 0.4465 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [384] | T 0.000079 | V 0.4444 V_ACC 92.31 takes 0.66 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [385] | T 0.000084 | V 0.4413 V_ACC 92.31 takes 0.29 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [386] | T 0.000097 | V 0.4399 V_ACC 92.31 takes 0.39 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [387] | T 0.000077 | V 0.4467 V_ACC 92.31 takes 0.55 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [388] | T 0.000086 | V 0.4427 V_ACC 92.31 takes 0.27 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [389] | T 0.000067 | V 0.4491 V_ACC 92.31 takes 0.35 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [390] | T 0.000054 | V 0.4446 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [391] | T 0.000066 | V 0.4459 V_ACC 92.31 takes 0.20 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [392] | T 0.000072 | V 0.4443 V_ACC 92.31 takes 0.33 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [393] | T 0.000085 | V 0.444 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [394] | T 0.000066 | V 0.4424 V_ACC 92.31 takes 0.42 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [395] | T 0.000084 | V 0.4406 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [396] | T 0.000078 | V 0.4398 V_ACC 92.31 takes 0.28 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [397] | T 0.000069 | V 0.4434 V_ACC 92.31 takes 0.36 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [398] | T 0.000071 | V 0.4366 V_ACC 92.31 takes 0.24 seconds\n",
            "\n",
            "26/26: [================>...............] - ETA 0.0s\n",
            "Epoch [399] | T 0.000065 | V 0.4373 V_ACC 92.31 takes 0.20 seconds\n",
            "\n",
            "Training has been finished.\n",
            "Save optimum models...\n",
            "Maximum Accuracy is at 11.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./Spk_Recog_14_6.12_SPK_RECOG_MODEL_Categorical_CE/chkpt_opt.pt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZjeuNTlirZZ"
      },
      "source": [
        "# 모델 평가 및 결과 분석"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhxtDWyuixkA"
      },
      "source": [
        "다음은 훈련 중 loss와 accuracy가 어떻게 변했는지를 그래프로 나타내보겠습니다.\n",
        "\n",
        "학습 과정이 길다면 한 눈에 살펴볼 수 있는 좋은 방법입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQotUfTmitex",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "c783dc3e-3751-4cd5-e6fc-de74d7009383"
      },
      "source": [
        "fp = open('./Spk_Recog_12_6.12_SPK_RECOG_MODEL_Categorical_CE/log.txt', 'r')\n",
        "line = fp.readlines()\n",
        "\n",
        "train_loss = np.zeros((len(line)-2, ))\n",
        "valid_loss = np.zeros((len(line)-2, ))\n",
        "valid_acc = np.zeros((len(line)-2, ))\n",
        "\n",
        "for idx in range(2, len(line)):\n",
        "    train_loss[idx-2] = line[idx].split(' ')[4]\n",
        "    valid_loss[idx-2] = line[idx].split(' ')[7]\n",
        "    valid_acc[idx-2] = line[idx].split(' ')[9]\n",
        "\n",
        "fp.close()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Loss')\n",
        "plt.plot(train_loss, 'b', label='train')\n",
        "plt.plot(valid_loss, 'r', label='valid')\n",
        "# plt.xlim([0, 100]); plt.ylim([0, 5])\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Accuracy')\n",
        "plt.plot(valid_acc, 'r', label='valid')\n",
        "plt.ylim([0, 100]); # plt.xlim([0, 100])\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wo_CN5XKjSxU"
      },
      "source": [
        "# [라벨링 순서]\n",
        "\n",
        "# '1'     , '2'     , '3'     , '4'     , '5'     , '6'     , '7'     , '8'     , '9'     , '10'    , '11'    , '12'    ,'13'    , '14'    , '15'    , '16'    , '17'    , '18'    , '19'    , '20'    '21'    , '22',   '23',   '24', '25',  '26'\n",
        "# '박경원', '김동현', '이수연',    '정재원',  '박성환',  '심형준',   '유지현',   '장종빈',  '임현우',   '안제호',   '박세진',  '이민구', '김장현',  '류승현',  '고무현',   '심대한',   '김영민',   '정성현',  '임태윤',  '진형민',   '김동원',   '변준', '박찬진', '차재빈', '허준영','신승민(조교)'\n",
        "\n",
        "\n",
        "Confusion matrix를 그려봅니다. (Training & Validation)\n",
        "\n",
        "Confusion matrix는 실제 정답과 예측한 정답 사이의 관계를 나타낸 표입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwgB_5t-jVvP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "828b262b-7a90-4ed7-a895-14b25d6ef764"
      },
      "source": [
        "# '1'     , '2'     , '3'     , '4'     , '5'     , '6'     , '7'     , '8'     , '9'     , '10'    , '11'    , '12'    , '13'    , '14'    , '15'    , '16'    , '17'    , '18'    , '19'    , '20'    , '21'    , '22'     \n",
        "# '박경원', '김동현', '이수연', '정재원', '박성환', '심형준', '유지현', '장종빈', '임현우', '안제호', '박세진', '이민구', '김장현', '류승현', '고무현', '심대한', '김영민', '정성현', '임태윤', '진형민', '김동원', '변준(조교)'\n",
        "import itertools\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "# Check point Settings\n",
        "CHECK_POINT_PATH = str('./Spk_Recog_12_6.12_SPK_RECOG_MODEL_Categorical_CE/chkpt_199.pt')\n",
        "checkpoint = torch.load(CHECK_POINT_PATH)\n",
        "\n",
        "\n",
        "# Model Load\n",
        "model = SPK_RECOG_MODEL().to(DEVICE)\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()\n",
        "\n",
        "# Load data (Validation data)\n",
        "print('Load data...')\n",
        "X_valid = pickle.load(open('./data/mfcc_features_devel', 'rb'))\n",
        "Y_valid = pickle.load(open('./data/labels_devel', 'rb'))\n",
        "\n",
        "\n",
        "X_valid = np.transpose(X_valid, (0, 2, 1))\n",
        "X_valid = torch.from_numpy(X_valid).float().to(DEVICE)\n",
        "\n",
        "\n",
        "y_pred = model(X_valid).to('cpu').detach().numpy()\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_gt = Y_valid.squeeze(1)\n",
        "\n",
        "# '1'     , '2'     , '3'     , '4'     , '5'     , '6'     , '7'     , '8'     , '9'     , '10'    , '11'    , '12'    ,\n",
        "# '박경원', '김동현', '이수연',    '정재원',  '박성환',  '심형준',   '유지현',   '장종빈',  '임현우',   '안제호',   '박세진',  '이민구',\n",
        "# '13'    , '14'    , '15'    , '16'    , '17'    , '18'    , '19'    , '20'    , '21'    , '22',   '23',   '24',   '25',   '26'\n",
        "#'김장현',  '류승현',  '고무현',   '심대한',   '김영민',   '정성현',  '임태윤',  '진형민',   '김동원',   '변준', '박찬진', '차재빈', '허준영','신승민(조교)'\n",
        "plt.figure(figsize=(8, 8))\n",
        "cm2 = confusion_matrix(y_gt, y_pred, labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21, 22, 23, 24, 25, 26])\n",
        "plt.imshow(cm2, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title(\"Confusion Matrix (Validation set)\")\n",
        "plt.colorbar()\n",
        "tick_marks = np.arange(26)\n",
        "plt.xticks(tick_marks, ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26'], rotation=45)\n",
        "plt.yticks(tick_marks, ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26'])\n",
        "thresh2 = cm2.max()/2.\n",
        "normalize = False\n",
        "fmt = '.2f' if normalize else 'd'\n",
        "for i, j in itertools.product(range(cm2.shape[0]), range(cm2.shape[1])):\n",
        "  plt.text(j, i, format(cm2[i,j], fmt), horizontalalignment=\"center\", color=\"white\" if cm2[i, j] > thresh2 else \"black\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Load data...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}